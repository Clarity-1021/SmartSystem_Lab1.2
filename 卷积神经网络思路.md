# pytorch

## 卷积层

#### 一、用法

- ##### Conv2d(in_channels, out_channels, kernel_size, stride=1,padding=0, dilation=1, groups=1,bias=True, padding_mode=‘zeros’)

#### 二、参数

- ##### in_channels：输入的通道数目 【必选】

- ##### out_channels： 输出的通道数目 【必选】

- ##### kernel_size：卷积核的大小，类型为int 或者元组，当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示 高和宽。【必选】

- ##### stride： 卷积每次滑动的步长为多少，默认是 1 【可选】

- ##### padding： 设置在所有边界增加 值为 0 的边距的大小（也就是在feature map 外围增加几圈 0 ），例如当 padding =1 的时候，如果原来大小为 3 × 3 ，那么之后的大小为 5 × 5 。即在外围加了一圈 0 。【可选】

- ##### dilation：控制卷积核之间的间距（什么玩意？请看例子）【可选】

如果我们设置的dilation=0的话，效果如图：（蓝色为输入，绿色为输出，卷积核为3 × 3）
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200130105522313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4ODYzNDEz,size_16,color_FFFFFF,t_70)
如果设置的是dilation=1，那么效果如图：（蓝色为输入，绿色为输出，卷积核仍为 3 × 3 。）
但是这里卷积核点与输入之间距离为1的值相乘来得到输出。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200130105658486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4ODYzNDEz,size_16,color_FFFFFF,t_70)

- ##### groups：控制输入和输出之间的连接。（不常用）【可选】

  举例来说：
  比如 groups 为1，那么所有的输入都会连接到所有输出
  当 groups 为 2的时候，相当于将输入分为两组，并排放置两层，每层看到一半的输入通道并产生一半的输出通道，并且两者都是串联在一起的。这也是参数字面的意思：“组” 的含义。
  需要注意的是，in_channels 和 out_channels 必须都可以整除 groups，否则会报错（因为要分成这么多组啊，除不开你让人家程序怎么办？）

- ##### bias： 是否将一个 学习到的 bias 增加输出中，默认是 True 。【可选】

- ##### padding_mode ： 字符串类型，接收的字符串只有 “zeros” 和 “circular”。【可选】

注意：参数 kernel_size，stride，padding，dilation 都可以是一个整数或者是一个元组，一个值的情况将会同时作用于**高和宽** 两个维度，两个值的元组情况代表分别作用于 **高** 和 **宽** 维度。

#### 三、相关形状

假设输入形状为：
$$
(N, C_{\text{in}}, H, W)
$$
则输出形状为：
$$
(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})
$$
其中$H_{out}$为：
$$
H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
$$
$W_{out}$ 为：
$$
W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
$$

#### 四、示例

入门学习者请不要过度关注某一些细节，建立一个简单的卷积层使用这个 `API` 其实很简单，大部分参数保持默认值就好，下面是简单的一个示例，创建一个简单的卷积神经网络：

```python
class CNN(nn.Module):
    def __init__(self,in_channels:int,out_channels:int):
        """
        创建一个卷积神经网络
        网络只有两层
        :param in_channels: 输入通道数量
        :param out_channels: 输出通道数量
        """
        super(CNN).__init__()
        self.conv1=nn.Conv2d(in_channels,10,3,stride=1,padding=1)
        self.pool1=nn.MaxPool2d(kernel_size=2,stride=1)
        self.conv2=nn.Conv2d(10,out_channels,3,stride=1,padding=1)
        self.pool2=nn.MaxPool2d(kernel_size=2,stride=1)
    def forward(self,x):
        """
        前向传播函数
        :param x:  输入，tensor 类型
        :return: 返回结果
        """
        out=self.conv1(x)
        out=self.pool1(out)
        out=self.conv2(out)
        out=self.pool2(out)
        return out
```

//官方文档

### class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)

二维卷积层, 输入的尺度是$(N, C_in,H,W)$，输出尺度$(N,C_out,H_out,W_out)$的计算方式：

$$out(N_i, C_{out_j})=bias(C_{out_j})+\sum^{C_{in}-1}*{k=0}weight(C*{out_j},k)\bigotimes input(N_i,k)$$

**说明**
`bigotimes`: 表示二维的相关系数计算 `stride`: 控制相关系数的计算步长
`dilation`: 用于控制内核点之间的距离，详细描述在[这里](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)
`groups`: 控制输入和输出之间的连接： `group=1`，输出是所有的输入的卷积；`group=2`，此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，并且产生的输出是输出通道的一半，随后将这两个输出连接起来。

参数`kernel_size`，`stride,padding`，`dilation`也可以是一个`int`的数据，此时卷积height和width值相同;也可以是一个`tuple`数组，`tuple`的第一维度表示height的数值，tuple的第二维度表示width的数值

**Parameters：**

- in_channels(`int`) – 输入信号的通道
- out_channels(`int`) – 卷积产生的通道
- kerner_size(`int` or `tuple`) - 卷积核的尺寸
- stride(`int` or `tuple`, `optional`) - 卷积步长
- padding(`int` or `tuple`, `optional`) - 输入的每一条边补充0的层数
- dilation(`int` or `tuple`, `optional`) – 卷积核元素之间的间距
- groups(`int`, `optional`) – 从输入通道到输出通道的阻塞连接数
- bias(`bool`, `optional`) - 如果`bias=True`，添加偏置

**shape:**
input: $(N,C_in,H_in,W_in)$
output:$ (N,C_out,H_out,W_out)$
$$
H_{out}=floor((H_{in}+2*padding[0]-dilation[0]*(kernerl_size[0]-1)-1)/stride[0]+1)​
$$

$$
W_{out}=floor((W_{in}+2*padding[1]-dilation[1]*(kernerl_size[1]-1)-1)/stride[1]+1)
$$

**变量:**
weight(`tensor`) - 卷积的权重，大小是(`out_channels`, `in_channels`,`kernel_size`)
bias(`tensor`) - 卷积的偏置系数，大小是（`out_channel`）

**example:**

```
>>> # With square kernels and equal stride
>>> m = nn.Conv2d(16, 33, 3, stride=2)
>>> # non-square kernels and unequal stride and with padding
>>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
>>> # non-square kernels and unequal stride and with padding and dilation
>>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
>>> input = autograd.Variable(torch.randn(20, 16, 50, 100))
>>> output = m(input)
```

## 池化层





//官方文档

### class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)

对于输入信号的输入通道，提供2维最大池化（`max pooling`）操作

如果输入的大小是$(N,C,H,W)$，那么输出的大小是$(N,C,H_{out},W_{out})$和池化窗口大小(kH,kW)的关系是：
$$out(N_i, C_j,k)=max^{kH-1}*{m=0}max^{kW-1}*{m=0}input(N_{i},C_j,stride[0]*h+m,stride[1]*w+n)$$

如果`padding`不是0，会在输入的每一边添加相应数目0
`dilation`用于控制内核点之间的距离，详细描述在[这里](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)

参数`kernel_size`，`stride`, `padding`，`dilation`数据类型： 可以是一个`int`类型的数据，此时卷积height和width值相同; 也可以是一个`tuple`数组（包含来两个int类型的数据），第一个`int`数据表示height的数值，`tuple`的第二个int类型的数据表示width的数值

**参数：**

- kernel_size(`int` or `tuple`) - max pooling的窗口大小
- stride(`int` or `tuple`, `optional`) - max pooling的窗口移动的步长。默认值是`kernel_size`
- padding(`int` or `tuple`, `optional`) - 输入的每一条边补充0的层数
- dilation(`int` or `tuple`, `optional`) – 一个控制窗口中元素步幅的参数
- return_indices - 如果等于`True`，会返回输出最大值的序号，对于上采样操作会有帮助
- ceil_mode - 如果等于`True`，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作

**shape:**
输入:$ (N,C,H_{in},W_in)$
输出: $(N,C,H_out,W_out)$
$$
H_{out}=floor((H_{in} + 2*padding[0] - dilation[0]*(kernel_size[0] - 1) - 1)/stride[0] + 1
$$

$$
W_{out}=floor((W_{in} + 2*padding[1] - dilation[1]*(kernel_size[1] - 1) - 1)/stride[1] + 1
$$

## yyds

https://shenxiaohai.me/2018/10/18/pytorch-tutorial-intermediate-01/